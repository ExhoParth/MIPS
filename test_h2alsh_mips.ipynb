{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from algorithms.h2alsh_mips import H2ALSH_MIPS  # Assuming your H2ALSH_MIPS implementation is saved in h2alsh_mips.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synethtic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 candidates for each query (indices):\n",
      "[[33 27 22  9  2]]\n",
      "Sample complexity (total number of operations): 17198\n",
      "Top 5 candidates using naive approach (indices): [21 33 27 38 45]\n",
      "Accuracy compared to naive method: 40.00%\n",
      "Speedup ratio: 0.29 times faster than naive computation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate synthetic test data\n",
    "def generate_test_data(num_atoms, len_signal):\n",
    "    \"\"\"\n",
    "    Generate random test data for atoms and signal vectors.\n",
    "    :param num_atoms: Number of atoms (data points)\n",
    "    :param len_signal: Length of each vector\n",
    "    :return: Tuple of atoms and signals\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # Set random seed for reproducibility\n",
    "    atoms = np.random.randn(num_atoms, len_signal)\n",
    "    signals = np.random.randn(1, len_signal)  # Single signal\n",
    "    return atoms, signals\n",
    "\n",
    "# Parameters for testing\n",
    "num_atoms = 100  # Number of atoms\n",
    "len_signal = 50  # Length of each vector\n",
    "atoms, signals = generate_test_data(num_atoms, len_signal)\n",
    "\n",
    "# Initialize H2-ALSH MIPS object\n",
    "h2alsh_mips = H2ALSH_MIPS(\n",
    "    atoms=atoms,\n",
    "    delta=0.1,  # Error probability\n",
    "    c0=2.0,     # Approximation constant c0 (c0-ANN problem)\n",
    "    c=0.9,      # Approximation constant c (c-AMIP problem)\n",
    "    N0=50       # Threshold for linear scan\n",
    ")\n",
    "\n",
    "# Run the H2-ALSH algorithm to retrieve top-k candidates\n",
    "top_k = 5\n",
    "candidates, sample_complexity = h2alsh_mips.mip_search_queries(signals, top_k=top_k)\n",
    "\n",
    "# Output results\n",
    "print(f\"Top {top_k} candidates for each query (indices):\\n{candidates}\")\n",
    "print(f\"Sample complexity (total number of operations): {sample_complexity.sum()}\")\n",
    "\n",
    "# Naive approach for validation\n",
    "inner_products = np.dot(atoms, signals[0])\n",
    "top_k_naive = np.argsort(inner_products)[-top_k:][::-1]  # Indices of top-k by brute force\n",
    "print(f\"Top {top_k} candidates using naive approach (indices): {top_k_naive}\")\n",
    "\n",
    "# Compare results\n",
    "accuracy = len(np.intersect1d(candidates[0], top_k_naive)) / top_k\n",
    "print(f\"Accuracy compared to naive method: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute speedup ratio\n",
    "total_naive_computations = num_atoms * len_signal * len(signals)\n",
    "speedup_ratio = total_naive_computations / sample_complexity.sum()\n",
    "print(f\"Speedup ratio: {speedup_ratio:.2f} times faster than naive computation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Lens 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 candidates (indices): [301 286 160 265   1 200  80  98 167  83]\n",
      "Sample complexity (total number of operations): 45203\n",
      "Top 10 candidates using naive approach (indices): [301 286 160 265   1 200  80  98 167  83]\n",
      "Accuracy compared to naive method: 100.00%\n",
      "Speedup ratio: 1.83 times faster than naive computation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from algorithms.h2alsh_mips import H2ALSH_MIPS  # Assuming H2ALSH_MIPS is implemented\n",
    "\n",
    "# Load the MovieLens dataset (100k version for simplicity)\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Build a user-item matrix\n",
    "user_item_matrix = np.zeros((trainset.n_users, trainset.n_items))\n",
    "for uid, iid, rating in trainset.all_ratings():\n",
    "    user_item_matrix[int(uid), int(iid)] = rating\n",
    "\n",
    "# Use SVD to reduce dimensionality\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "atoms = svd.fit_transform(user_item_matrix.T)  # Transpose to get item embeddings\n",
    "\n",
    "# Create a signal (simulate a user's preference vector)\n",
    "user_id = 0  # Choose a user ID from the dataset\n",
    "user_ratings = user_item_matrix[user_id]\n",
    "signal = np.dot(user_ratings, atoms)  # Weighted average of rated item embeddings\n",
    "\n",
    "# Initialize H2-ALSH MIPS object\n",
    "h2alsh_mips = H2ALSH_MIPS(\n",
    "    atoms=atoms,\n",
    "    delta=0.1,  # Error probability\n",
    "    c0=2.0,     # Approximation constant for c0-ANN problem\n",
    "    c=0.9,      # Approximation constant for c-AMIP problem\n",
    "    N0=50       # Threshold for linear scan\n",
    ")\n",
    "\n",
    "# Run the H2-ALSH algorithm to retrieve top-k candidates\n",
    "top_k = 10\n",
    "candidates, sample_complexity = h2alsh_mips.mip_search(signal, top_k=top_k)\n",
    "\n",
    "# Output results\n",
    "print(f\"Top {top_k} candidates (indices): {candidates}\")\n",
    "print(f\"Sample complexity (total number of operations): {sample_complexity}\")\n",
    "\n",
    "# Naive approach for validation\n",
    "inner_products = np.dot(atoms, signal)\n",
    "top_k_naive = np.argsort(inner_products)[-top_k:][::-1]  # Indices of top-k by brute force\n",
    "print(f\"Top {top_k} candidates using naive approach (indices): {top_k_naive}\")\n",
    "\n",
    "# Compare results\n",
    "accuracy = len(np.intersect1d(candidates, top_k_naive)) / top_k\n",
    "print(f\"Accuracy compared to naive method: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute speedup ratio\n",
    "total_naive_computations = atoms.shape[0] * atoms.shape[1]  # Total naive computations\n",
    "speedup_ratio = total_naive_computations / sample_complexity\n",
    "print(f\"Speedup ratio: {speedup_ratio:.2f} times faster than naive computation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_item_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie lens 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 candidates (indices): [1136  434  182  435   85  207  118   69   52  303]\n",
      "Sample complexity (total number of operations): 45203\n",
      "Top 10 candidates using naive approach (indices): [1136  434  182  435   85  207  118   69   52  303]\n",
      "Accuracy compared to naive method: 100.00%\n",
      "Speedup ratio: 4.06 times faster than naive computation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from algorithms.h2alsh_mips import H2ALSH_MIPS  # Assuming H2ALSH_MIPS is implemented\n",
    "\n",
    "# Load the MovieLens dataset (100k version for simplicity)\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "data = Dataset.load_builtin('ml-1m')\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Build a user-item matrix\n",
    "user_item_matrix = np.zeros((trainset.n_users, trainset.n_items))\n",
    "for uid, iid, rating in trainset.all_ratings():\n",
    "    user_item_matrix[int(uid), int(iid)] = rating\n",
    "\n",
    "# Use SVD to reduce dimensionality\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "atoms = svd.fit_transform(user_item_matrix.T)  # Transpose to get item embeddings\n",
    "\n",
    "# Create a signal (simulate a user's preference vector)\n",
    "user_id = 0  # Choose a user ID from the dataset\n",
    "user_ratings = user_item_matrix[user_id]\n",
    "signal = np.dot(user_ratings, atoms)  # Weighted average of rated item embeddings\n",
    "\n",
    "# Initialize H2-ALSH MIPS object\n",
    "h2alsh_mips = H2ALSH_MIPS(\n",
    "    atoms=atoms,\n",
    "    delta=0.1,  # Error probability\n",
    "    c0=2.0,     # Approximation constant for c0-ANN problem\n",
    "    c=0.9,      # Approximation constant for c-AMIP problem\n",
    "    N0=50       # Threshold for linear scan\n",
    ")\n",
    "\n",
    "# Run the H2-ALSH algorithm to retrieve top-k candidates\n",
    "top_k = 10\n",
    "candidates, sample_complexity = h2alsh_mips.mip_search(signal, top_k=top_k)\n",
    "\n",
    "# Output results\n",
    "print(f\"Top {top_k} candidates (indices): {candidates}\")\n",
    "print(f\"Sample complexity (total number of operations): {sample_complexity}\")\n",
    "\n",
    "# Naive approach for validation\n",
    "inner_products = np.dot(atoms, signal)\n",
    "top_k_naive = np.argsort(inner_products)[-top_k:][::-1]  # Indices of top-k by brute force\n",
    "print(f\"Top {top_k} candidates using naive approach (indices): {top_k_naive}\")\n",
    "\n",
    "# Compare results\n",
    "accuracy = len(np.intersect1d(candidates, top_k_naive)) / top_k\n",
    "print(f\"Accuracy compared to naive method: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute speedup ratio\n",
    "total_naive_computations = atoms.shape[0] * atoms.shape[1]  # Total naive computations\n",
    "speedup_ratio = total_naive_computations / sample_complexity\n",
    "print(f\"Speedup ratio: {speedup_ratio:.2f} times faster than naive computation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_item_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netflix Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 candidates (indices): [1274  738   46  559  314 1305  305   24  217 1041]\n",
      "Sample complexity (total number of operations): 203200\n",
      "Top 10 candidates using naive approach (indices): [1274  738   46  559  314 1171 1305  305   24  217]\n",
      "Accuracy compared to naive method: 90.00%\n",
      "Speedup ratio: 0.66 times faster than naive computation.\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed factors and biases\n",
    "movie_factors = np.load(\"data/netflix/Movie_factors_15_new.npy\")\n",
    "movie_biases = np.load(\"data/netflix/Movie_biases_15_new.npy\")\n",
    "customer_factors = np.load(\"data/netflix/Customer_factors_15_new.npy\")\n",
    "customer_biases = np.load(\"data/netflix/Customer_biases_15_new.npy\")\n",
    "global_mean = np.load(\"data/netflix/netflix_global_mean.npy\")\n",
    "\n",
    "# Use movie factors as atoms for LSH-MIPS\n",
    "atoms = movie_factors\n",
    "\n",
    "# Choose a specific user\n",
    "user_id = 0  # Replace with the desired user ID (index-based, starting at 0)\n",
    "user_factors = customer_factors[user_id]\n",
    "user_bias = customer_biases[user_id]\n",
    "\n",
    "# Construct the user preference signal\n",
    "signal = user_factors  # Optionally, add user_bias and global_mean if needed for personalization\n",
    "\n",
    "# Initialize H2-ALSH MIPS object\n",
    "h2alsh_mips = H2ALSH_MIPS(\n",
    "    atoms=atoms,\n",
    "    delta=0.1,  # Error probability\n",
    "    c0=2.0,     # Approximation constant for c0-ANN problem\n",
    "    c=0.9,      # Approximation constant for c-AMIP problem\n",
    "    N0=50       # Threshold for linear scan\n",
    ")\n",
    "\n",
    "# Run the H2-ALSH algorithm to retrieve top-k candidates\n",
    "top_k = 10\n",
    "candidates, sample_complexity = h2alsh_mips.mip_search(signal, top_k=top_k)\n",
    "\n",
    "# Output results\n",
    "print(f\"Top {top_k} candidates (indices): {candidates}\")\n",
    "print(f\"Sample complexity (total number of operations): {sample_complexity}\")\n",
    "\n",
    "# Naive approach for validation\n",
    "inner_products = np.dot(atoms, signal)\n",
    "top_k_naive = np.argsort(inner_products)[-top_k:][::-1]  # Indices of top-k by brute force\n",
    "print(f\"Top {top_k} candidates using naive approach (indices): {top_k_naive}\")\n",
    "\n",
    "# Compare results\n",
    "accuracy = len(np.intersect1d(candidates, top_k_naive)) / top_k\n",
    "print(f\"Accuracy compared to naive method: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute speedup ratio\n",
    "total_naive_computations = atoms.shape[0] * atoms.shape[1]  # Total naive computations\n",
    "speedup_ratio = total_naive_computations / sample_complexity\n",
    "print(f\"Speedup ratio: {speedup_ratio:.2f} times faster than naive computation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crypto-Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 candidates (indices): [ 58  97  35 104  49  77  67  33  19 101]\n",
      "Sample complexity (total number of operations): 18034\n",
      "Top 10 candidates using naive approach (indices): [ 13  28  51   2  58  97  35 104  57  49]\n",
      "Accuracy compared to naive method: 50.00%\n",
      "Speedup ratio: 0.29 times faster than naive computation.\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset\n",
    "dataset_path = \"data\\crypto_pairs\\crypto_pairs_1m_dimensions.npy\"  # Path to the saved .npy file\n",
    "crypto_data = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "# Step 1: Use Truncated SVD to reduce dimensionality (if necessary)\n",
    "svd = TruncatedSVD(n_components=50)  # Reduce to 50 dimensions\n",
    "atoms = svd.fit_transform(crypto_data)  # The reduced dataset\n",
    "\n",
    "# Step 2: Create a query signal\n",
    "# Example: Use the first crypto pair as the query vector\n",
    "query_index = 0\n",
    "signal = atoms[query_index]  # A single crypto pair's embedding\n",
    "\n",
    "# Initialize H2-ALSH MIPS object\n",
    "h2alsh_mips = H2ALSH_MIPS(\n",
    "    atoms=atoms,\n",
    "    delta=0.1,  # Error probability\n",
    "    c0=2.0,     # Approximation constant for c0-ANN problem\n",
    "    c=0.9,      # Approximation constant for c-AMIP problem\n",
    "    N0=50       # Threshold for linear scan\n",
    ")\n",
    "\n",
    "# Run the H2-ALSH algorithm to retrieve top-k candidates\n",
    "top_k = 10\n",
    "candidates, sample_complexity = h2alsh_mips.mip_search(signal, top_k=top_k)\n",
    "\n",
    "# Output results\n",
    "print(f\"Top {top_k} candidates (indices): {candidates}\")\n",
    "print(f\"Sample complexity (total number of operations): {sample_complexity}\")\n",
    "\n",
    "# Naive approach for validation\n",
    "inner_products = np.dot(atoms, signal)\n",
    "top_k_naive = np.argsort(inner_products)[-top_k:][::-1]  # Indices of top-k by brute force\n",
    "print(f\"Top {top_k} candidates using naive approach (indices): {top_k_naive}\")\n",
    "\n",
    "# Compare results\n",
    "accuracy = len(np.intersect1d(candidates, top_k_naive)) / top_k\n",
    "print(f\"Accuracy compared to naive method: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute speedup ratio\n",
    "total_naive_computations = atoms.shape[0] * atoms.shape[1]  # Total naive computations\n",
    "speedup_ratio = total_naive_computations / sample_complexity\n",
    "print(f\"Speedup ratio: {speedup_ratio:.2f} times faster than naive computation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
